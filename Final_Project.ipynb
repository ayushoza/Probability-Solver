{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO4gWUSfXMOJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "from fractions import Fraction\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
        "from torchtext import data\n",
        "\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# The first time you run this will download a ~823MB file\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
        "                              dim=300)   # embedding size = 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Nnlkp4ateavp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/gdrive/My Drive/UTM/CSC413/Final Project/\"\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "IvuZW98_eblG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_data(t):\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    tokens_from_tokenizer = tokenizer(t)\n",
        "    tokens = []\n",
        "    for word in tokens_from_tokenizer:\n",
        "        if word in glove.stoi:\n",
        "          tokens.append(word)\n",
        "        else: \n",
        "          tokens.extend(word)\n",
        "    return tokens\n",
        "\n",
        "#parse_data(glove, \"What is prob of picking 1 b and 1 p when two letters picked without replacement from tpppbbpbbb?\")\n",
        "#parse_data(glove, \"What is prob of picking 1 p and 1 y when two letters picked without replacement from {y: 1, p: 2, z: 1, n: 2}?\")"
      ],
      "metadata": {
        "id": "4-yhcUeOmJ30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data_path, batch_size=32, device=\"cpu\", embedding=\"glove.6B.300d\", shuffle=True):\n",
        "    questions = Field(sequential=True, use_vocab=True, tokenize=parse_data, lower=True)\n",
        "    solutions = Field(sequential=False, use_vocab=False, dtype=torch.float)\n",
        "    # Create fields\n",
        "    fields = {'question': ('q', questions), 'solution': ('s', solutions)}\n",
        "\n",
        "    # Obtain datasets\n",
        "    train_data, valid_data, test_data = TabularDataset.splits(path=data_path, train=\"train(smaller).json\", validation=\"valid(smaller).json\",\n",
        "                                               test='test(smaller).json', format='json', fields=fields)\n",
        "\n",
        "    # Build vocab object\n",
        "    questions.build_vocab(train_data, max_size=10000, min_freq=1, vectors=embedding)\n",
        "\n",
        "    # Obtain iterators for batch training (dataset: tuple, batch_size: tuple) (Must be 1 to 1)\n",
        "    train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "        (train_data, valid_data, test_data), batch_sizes=(batch_size, batch_size, batch_size), device=device, shuffle=shuffle, sort=False\n",
        "    )\n",
        "    return train_data, valid_data, test_data, train_iterator, valid_iterator, test_iterator, questions\n",
        "\n",
        "\n",
        "# Obtain all data and iterators\n",
        "train_data, valid_data, test_data, train_iterator, valid_iterator, test_iterator, questions_field = preprocess(data_path, batch_size=256, device=device, shuffle=True)"
      ],
      "metadata": {
        "id": "aoiLS-1hVw1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, data):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in data:\n",
        "        src_mask = generate_square_subsequent_mask(len(batch.q)).to(device)\n",
        "        # Move output to cpu for error calculation\n",
        "        output = model(batch.q, src_mask).cpu()\n",
        "        # Move labels to cpu for error calculation\n",
        "        labels = batch.s.cpu()\n",
        "        # tolerance = 1e-3     i.e. 0.001 difference between label and output\n",
        "        correct += sum(np.isclose(output.detach().numpy(), labels, 1e-3,1e-3)) # n in 10**-n is the number of decimal places\n",
        "        total += len(batch.s)\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "X7PnyJo6rq1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_data, valid_data, weight_decay=0.0,\n",
        "           learning_rate=0.005, num_epochs=50):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=learning_rate,\n",
        "                           weight_decay=weight_decay)\n",
        "\n",
        "    iters_sub, train_accs, val_accs  = [], [] ,[]\n",
        "    checkpoint_path = '/content/gdrive/My Drive/CSC413/Final Project/ckpt'\n",
        "    \n",
        "    #src_mask = generate_square_subsequent_mask(44).to(device)\n",
        "    batches = 0\n",
        "    # Added tqdm to track progression\n",
        "    for n in tqdm(range(1, num_epochs+1)):\n",
        "        # Obtain loss per epoch\n",
        "        temp_loss = None\n",
        "        for batch in train_iterator:\n",
        "            questions = batch.q.to(device)\n",
        "            answers = batch.s.to(device)\n",
        "            src_mask = generate_square_subsequent_mask(len(questions)).to(device)\n",
        "            model.train()\n",
        "            zs = model(questions, src_mask)\n",
        "            loss = criterion(zs, answers) # compute the total loss\n",
        "            temp_loss = loss\n",
        "            loss.backward()          # compute updates for each parameter\n",
        "            optimizer.step()         # make the updates for each parameter\n",
        "            optimizer.zero_grad()    # a clean up step for PyTorch\n",
        "\n",
        "        # save the current training information\n",
        "        temp_loss = temp_loss.cpu().detach().item()\n",
        "        losses.append(temp_loss)\n",
        "        iters_sub.append(n)\n",
        "        train_acc = get_accuracy(model, train_iterator)\n",
        "        train_accs.append(train_acc)\n",
        "        val_acc = get_accuracy(model, valid_iterator)\n",
        "        val_accs.append(val_acc)\n",
        "        print(\"\\nEpoch: {}\\nTrain Acc: {} %       Valid Acc: {} % \\nTrain Loss: {}\".format(n, round(train_acc, 5)*100, round(val_acc, 5)*100, temp_loss))\n",
        "        # print(\"Epoch %d. [Val Acc %.3f%%] [Train Acc %.3f%%]\" % (\n",
        "              # n, val_acc * 100, train_acc * 100))\n",
        "\n",
        "        if (checkpoint_path is not None) and n % 200 == 0:\n",
        "            torch.save(model.state_dict(), checkpoint_path + str(n) + \".dat\")\n",
        "    \n",
        "    plt.title(\"Learning Curve: Accuracy per Epoch\")\n",
        "    plt.plot(iters_sub, train_accs, label=\"Train\")\n",
        "    # plt.plot(iters_sub, val_accs, label=\"Validation\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "nv7u1dvk88dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, 0)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, 4096)\n",
        "        self.layer1 = nn.Linear(4096, 4096)\n",
        "        self.layer2 = nn.Linear(4096, 2048)\n",
        "        self.layer3 = nn.Linear(2048, 2048)\n",
        "        self.layer4 = nn.Linear(2048, 1024)\n",
        "        self.layer5 = nn.Linear(1024, 1024)\n",
        "        self.layer6 = nn.Linear(1024, 512)\n",
        "        self.layer7 = nn.Linear(512, 512)\n",
        "        self.layer8 = nn.Linear(512, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.layer1.weight.data.uniform_(-initrange, initrange)\n",
        "        self.layer1.bias.data.zero_()\n",
        "        self.layer2.weight.data.uniform_(-initrange, initrange)\n",
        "        self.layer2.bias.data.zero_()\n",
        "        self.layer3.weight.data.uniform_(-initrange, initrange)\n",
        "        self.layer3.bias.data.zero_()\n",
        "        self.layer4.weight.data.uniform_(-initrange, initrange)\n",
        "        self.layer4.bias.data.zero_()\n",
        "        self.layer5.weight.data.uniform_(-initrange, initrange)\n",
        "        self.layer5.bias.data.zero_()\n",
        "        self.layer6.weight.data.uniform_(-initrange, initrange)\n",
        "        self.layer6.bias.data.zero_()\n",
        "        self.layer7.weight.data.uniform_(-initrange, initrange)\n",
        "        self.layer7.bias.data.zero_()\n",
        "        self.layer8.weight.data.uniform_(-initrange, initrange)\n",
        "        self.layer8.bias.data.zero_()\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = torch.mean(output, 0)\n",
        "\n",
        "        output = self.decoder(output)\n",
        "        output = torch.relu(output)\n",
        "\n",
        "        output = self.layer1(output)\n",
        "        output = torch.relu(output)\n",
        "\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        output = self.layer2(output)\n",
        "        output = torch.relu(output)\n",
        "\n",
        "        output = self.layer3(output)\n",
        "        output = torch.relu(output)\n",
        "\n",
        "        output = self.layer4(output)\n",
        "        output = torch.relu(output)\n",
        "\n",
        "        output = self.layer5(output)\n",
        "        output = torch.relu(output)\n",
        "\n",
        "        output = self.layer6(output)\n",
        "        output = torch.relu(output)\n",
        "\n",
        "        output = self.layer7(output)\n",
        "        output = torch.relu(output)\n",
        "\n",
        "        output = self.layer8(output)\n",
        "\n",
        "        return torch.squeeze(output)\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ],
      "metadata": {
        "id": "NRkMSZn1cLOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_accuracy(model, data, tolerance):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in data:\n",
        "        src_mask = generate_square_subsequent_mask(len(batch.q)).to(device)\n",
        "        output = model(batch.q, src_mask).cpu().detach().numpy()\n",
        "        labels = batch.s.cpu()\n",
        "        correct += sum(np.isclose(output, labels, tolerance, tolerance)) # n in 10**-n is the number of decimal places\n",
        "        total += len(batch.s)\n",
        "    return correct / total\n",
        "\n",
        "emsize = 300  # embedding dimension\n",
        "ntoken = len(questions_field.vocab)\n",
        "d_hid = 1024  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 6  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 3  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.1  # dropout probability\n",
        "model = TransformerModel(ntoken, emsize, nhead, d_hid, nlayers, dropout)\n",
        "# Obtain previous params\n",
        "model.load_state_dict(torch.load(\"/content/gdrive/My Drive/UTM/CSC413/Final Project/ckpt2/1950.dat\"))\n",
        "# Dump to GPU after init\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "acc = get_test_accuracy(model,test_iterator, 1e-3)\n",
        "print(f'Test accuracy: {round(acc*100, 3)}')"
      ],
      "metadata": {
        "id": "pp4rdS6HElyI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}